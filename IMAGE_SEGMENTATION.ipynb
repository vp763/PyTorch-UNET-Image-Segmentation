{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# End-to-End Image Segmentation with U-Net in PyTorch\n",
        "\n"
      ],
      "metadata": {
        "id": "NZDoPJZO6Wh8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Designed, built, and trained a deep learning model from scratch for semantic segmentation of objects in the PASCAL VOC 2012 dataset. Developed a complete, end-to-end data pipeline, from data acquisition and preprocessing to model training, evaluation, and visualization in a cloud-based environment."
      ],
      "metadata": {
        "id": "hI18ag6L6zPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import glob\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import torchvision"
      ],
      "metadata": {
        "id": "mDXUPn_t9tip"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_HOsFEZY_xH",
        "outputId": "f0f02ec5-f2d8-4282-88c2-8ce3a98cfd45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Key from Google Drive is complete!\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive to access the Kaggle API key\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/drive/MyDrive/ZidioProject/kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "print(\"Key from Google Drive is complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exF1p7jhdjj-",
        "outputId": "fbe1bd9e-7b34-4df0-f85f-4ee6c3a10eee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/sovitrath/voc-2012-segmentation-data\n",
            "License(s): CC0-1.0\n",
            "Downloading voc-2012-segmentation-data.zip to ./data\n",
            " 93% 294M/316M [00:00<00:00, 729MB/s] \n",
            "100% 316M/316M [00:00<00:00, 784MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Download the PASCAL VOC 2012 dataset from Kaggle\n",
        "!kaggle datasets download -d sovitrath/voc-2012-segmentation-data -p ./data --unzip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Building the Data Pipeline"
      ],
      "metadata": {
        "id": "qF6kA7Zk7LVT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Bp4guukki1j",
        "outputId": "9434137a-2a9c-48ed-9691-32b8a6ea8233"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset ready with 1464 images.\n",
            "Shape of one BATCH of images: torch.Size([4, 3, 256, 256])\n",
            "Shape of one BATCH of masks: torch.Size([4, 1, 256, 256])\n"
          ]
        }
      ],
      "source": [
        "#paths\n",
        "train_image_path = f'./data/voc_2012_segmentation_data/train_images/'\n",
        "train_label_path = f'./data/voc_2012_segmentation_data/train_labels/'\n",
        "\n",
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(self, image_path, label_path):\n",
        "        self.images = sorted(glob.glob(os.path.join(image_path, '*.jpg')))\n",
        "        self.labels = sorted(glob.glob(os.path.join(label_path, '*.png')))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        image_path = self.images[idx]\n",
        "        label_path = self.labels[idx]\n",
        "\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        mask = Image.open(label_path).convert('L')\n",
        "\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((256, 256)),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "\n",
        "        #Prepareconvert to Tensor)\n",
        "        image = transform(image)\n",
        "        mask = transform(mask)\n",
        "\n",
        "        # return the pair\n",
        "        return image, mask\n",
        "\n",
        "# verification\n",
        "train_dataset = SegmentationDataset(train_image_path, train_label_path)\n",
        "print(f\"Dataset ready with {len(train_dataset)} images.\")\n",
        "\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=True,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "first_batch_images, first_batch_masks = next(iter(train_dataloader))\n",
        "\n",
        "print(f\"Shape of one BATCH of images: {first_batch_images.shape}\")\n",
        "print(f\"Shape of one BATCH of masks: {first_batch_masks.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: U-Net Model Architecture"
      ],
      "metadata": {
        "id": "s-vu6ZcC7o9j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Mr3kBccCyoAL"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "9_CqUAxXem2Q"
      },
      "outputs": [],
      "source": [
        "class UNET(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels=3, out_channels=1, features=[64, 128, 256, 512]):\n",
        "        super(UNET, self).__init__()\n",
        "\n",
        "        self.down = nn.ModuleList()\n",
        "        self.up = nn.ModuleList()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # encoder\n",
        "        for feature in features:\n",
        "            self.down.append(DoubleConv(in_channels, feature))\n",
        "            in_channels = feature\n",
        "\n",
        "        # decoder\n",
        "        for feature in reversed(features):\n",
        "            self.up.append(nn.ConvTranspose2d(feature*2, feature, kernel_size=2, stride=2))\n",
        "            self.up.append(DoubleConv(feature*2, feature))\n",
        "\n",
        "        # bottleneck\n",
        "        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
        "\n",
        "        # final conv\n",
        "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        skip_connections = []\n",
        "\n",
        "        #down the U-Net\n",
        "        for down_block in self.down:\n",
        "            x = down_block(x)\n",
        "            skip_connections.append(x)\n",
        "            x = self.pool(x)\n",
        "\n",
        "        # bottleneck\n",
        "        x = self.bottleneck(x)\n",
        "\n",
        "        skip_connections.reverse()\n",
        "\n",
        "        # Go up the U-Net\n",
        "        for idx in range(0, len(self.up), 2):\n",
        "            x = self.up[idx](x)\n",
        "            skip_connection_tensor = skip_connections[idx//2]\n",
        "\n",
        "            if x.shape != skip_connection_tensor.shape:\n",
        "                x = F.interpolate(x, size=skip_connection_tensor.shape[2:])\n",
        "\n",
        "            concat_tensor = torch.cat((skip_connection_tensor, x), dim=1)\n",
        "            x = self.up[idx+1](concat_tensor)\n",
        "\n",
        "        return self.final_conv(x)\n",
        "\n",
        "model = UNET(in_channels=3, out_channels=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Training the Model"
      ],
      "metadata": {
        "id": "wA-l5_ju8N9E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_PWZ5N4thWl",
        "outputId": "f5589718-0cfc-4ed8-81b8-b2c459135801"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final check, running on device: CUDA\n",
            "\n",
            "--- Epoch 1/35 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 92/92 [01:08<00:00,  1.35it/s, loss=0.39]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 2/35 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 92/92 [01:12<00:00,  1.27it/s, loss=0.389]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 3/35 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 92/92 [01:12<00:00,  1.27it/s, loss=0.425]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 4/35 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 92/92 [01:12<00:00,  1.27it/s, loss=0.295]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 5/35 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 92/92 [01:12<00:00,  1.26it/s, loss=0.38]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 6/35 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 92/92 [01:12<00:00,  1.26it/s, loss=0.392]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 7/35 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 92/92 [01:12<00:00,  1.26it/s, loss=0.312]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 8/35 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 92/92 [01:12<00:00,  1.26it/s, loss=0.31]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 9/35 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 92/92 [01:12<00:00,  1.26it/s, loss=0.247]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 10/35 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 92/92 [01:12<00:00,  1.26it/s, loss=0.363]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 11/35 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 92/92 [01:12<00:00,  1.26it/s, loss=0.424]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 12/35 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 92/92 [01:12<00:00,  1.26it/s, loss=0.426]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 13/35 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 92/92 [01:12<00:00,  1.26it/s, loss=0.261]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 14/35 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 92/92 [01:12<00:00,  1.26it/s, loss=0.325]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 15/35 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 92/92 [01:12<00:00,  1.26it/s, loss=0.351]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 16/35 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 92/92 [01:12<00:00,  1.26it/s, loss=0.673]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 17/35 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 92/92 [01:12<00:00,  1.26it/s, loss=0.336]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 18/35 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 92/92 [01:12<00:00,  1.26it/s, loss=0.276]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 19/35 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 92/92 [01:12<00:00,  1.26it/s, loss=0.258]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 20/35 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 92/92 [01:12<00:00,  1.26it/s, loss=0.366]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 21/35 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 92/92 [01:12<00:00,  1.26it/s, loss=0.209]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 22/35 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 92/92 [01:12<00:00,  1.26it/s, loss=0.27]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 23/35 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 92/92 [01:12<00:00,  1.26it/s, loss=0.296]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 24/35 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 92/92 [01:12<00:00,  1.26it/s, loss=0.318]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 25/35 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 92/92 [01:12<00:00,  1.26it/s, loss=0.311]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 26/35 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 92/92 [01:12<00:00,  1.26it/s, loss=0.3]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 27/35 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 92/92 [01:12<00:00,  1.26it/s, loss=0.289]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 28/35 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 92/92 [01:12<00:00,  1.26it/s, loss=0.194]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 29/35 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 92/92 [01:12<00:00,  1.26it/s, loss=0.299]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 30/35 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 92/92 [01:12<00:00,  1.26it/s, loss=0.245]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 31/35 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 92/92 [01:12<00:00,  1.26it/s, loss=0.337]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 32/35 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 92/92 [01:12<00:00,  1.26it/s, loss=0.404]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 33/35 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 92/92 [01:12<00:00,  1.26it/s, loss=0.222]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 34/35 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 92/92 [01:12<00:00,  1.26it/s, loss=0.22]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch 35/35 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 92/92 [01:12<00:00,  1.26it/s, loss=0.223]\n"
          ]
        }
      ],
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Final check, running on device: {DEVICE.upper()}\")\n",
        "\n",
        "LEARNING_RATE = 1e-4\n",
        "BATCH_SIZE = 16\n",
        "NUM_EPOCHS = 35\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "model = UNET(in_channels=3, out_channels=1).to(DEVICE)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "#training\n",
        "def train_fn(loader, model, optimizer, loss_fn, device):\n",
        "    model.train()\n",
        "    loop = tqdm(loader, leave=True)\n",
        "    for batch_idx, (data, targets) in enumerate(loop):\n",
        "        data = data.to(device=device)\n",
        "        targets = targets.float().to(device=device)\n",
        "\n",
        "        predictions = model(data)\n",
        "        loss = loss_fn(predictions, targets)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "\n",
        "#training loop\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\n--- Epoch {epoch+1}/{NUM_EPOCHS} ---\")\n",
        "    train_fn(train_loader, model, optimizer, loss_fn, DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Prepeare Test Data"
      ],
      "metadata": {
        "id": "iYTCObbr8nD6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McC0rYxL58JP",
        "outputId": "13e3ab1a-87f3-4be5-ff8f-b06e2ae6b11d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset ready with 1449 images.\n",
            "Shape of one BATCH of images: torch.Size([4, 3, 256, 256])\n",
            "Shape of one BATCH of masks: torch.Size([4, 1, 256, 256])\n"
          ]
        }
      ],
      "source": [
        "valid_image_path = f'./data/voc_2012_segmentation_data/valid_images/'\n",
        "valid_labels_path = f'./data/voc_2012_segmentation_data/valid_labels/'\n",
        "\n",
        "\n",
        "\n",
        "# --- Verification ---\n",
        "valid_dataset = SegmentationDataset(valid_image_path, valid_labels_path)\n",
        "print(f\"Dataset ready with {len(valid_dataset)} images.\")\n",
        "\n",
        "\n",
        "\n",
        "valid_dataloader = DataLoader(\n",
        "    dataset=valid_dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=False,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "first_batch_images, first_batch_masks = next(iter(valid_dataloader))\n",
        "\n",
        "print(f\"Shape of one BATCH of images: {first_batch_images.shape}\")\n",
        "print(f\"Shape of one BATCH of masks: {first_batch_masks.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Evaluating the Model"
      ],
      "metadata": {
        "id": "6lJ3Qk3r8eZS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "LI7EqS-b6snn"
      },
      "outputs": [],
      "source": [
        "def check_accuracy(loader, model, device=\"cuda\"):\n",
        "    num_correct = 0\n",
        "    num_pixels = 0\n",
        "    dice_score = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            preds = torch.sigmoid(model(x))\n",
        "            preds = (preds > 0.5).float()\n",
        "            num_correct += (preds == y).sum()\n",
        "            num_pixels += torch.numel(preds)\n",
        "\n",
        "            dice_score += (2 * (preds * y).sum()) / (\n",
        "                (preds + y).sum() + 1e-6)\n",
        "\n",
        "    print(f\"Got {num_correct}/{num_pixels} with accuracy {num_correct/num_pixels*100:.2f}%\")\n",
        "    avg_dice_score = dice_score/len(loader)\n",
        "    print(f\"Dice score: {avg_dice_score:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrQBL_M1-lnh",
        "outputId": "88f60a35-a409-40a5-ebfc-f4c7004b59a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got 63071576/94961664 with accuracy 66.42%\n",
            "Dice score: 0.2984\n"
          ]
        }
      ],
      "source": [
        "check_accuracy(valid_dataloader,model,DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Visualizing the Results"
      ],
      "metadata": {
        "id": "Jy74XmrJ8iUF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifh9tfRT-uxh",
        "outputId": "1c824008-98c6-45e1-aee5-6f8961d8ae46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving one batch of predictions to the 'saved_images' folder...\n",
            "Done. Check the file explorer on the left.\n"
          ]
        }
      ],
      "source": [
        "def save_predictions_as_imgs(loader, model, folder=\"saved_images/\", device=\"cuda\"):\n",
        "    model.eval()\n",
        "\n",
        "    # Get one batch from the loader\n",
        "    x, y = next(iter(loader))\n",
        "    x, y = x.to(device), y.to(device)\n",
        "\n",
        "    # Create the folder if it doesn't exist\n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        preds = torch.sigmoid(model(x))\n",
        "        preds = (preds > 0.5).float()\n",
        "\n",
        "    # Save the images, ground truth masks, and predictions\n",
        "    # Each will be a grid of images from the batch\n",
        "    torchvision.utils.save_image(\n",
        "        x, f\"{folder}/original_images.png\")\n",
        "    torchvision.utils.save_image(\n",
        "        y, f\"{folder}/true_masks.png\")\n",
        "    torchvision.utils.save_image(\n",
        "        preds, f\"{folder}/pred_masks.png\")\n",
        "\n",
        "    model.train()\n",
        "\n",
        "# --- Run the visualization ---\n",
        "print(\"Saving one batch of predictions to the 'saved_images' folder...\")\n",
        "save_predictions_as_imgs(valid_dataloader, model, device=DEVICE)\n",
        "print(\"Done. Check the file explorer on the left.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVcnJcSPBEUs"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}